"""Shared helpers for migrate_v2 smoke tests."""

from __future__ import annotations

import hashlib
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable

_SAMPLE_STRUCTURE = {
    "photos/2022/summer": ["beach.txt", "mountain.txt"],
    "photos/2023/winter": ["ski.txt", "snowman.txt"],
    "docs/reports": ["q1.md", "q2.md", "summary.md"],
    "logs/app/server": ["2024-01.log", "2024-02.log", "2024-03.log"],
    "data/json": ["users.json", "inventory.json"],
}


@dataclass(frozen=True)
class SmokeTestDeps:
    """Shared dependencies passed from migrate_v2."""

    config: Any
    drive_checker_fn: Callable
    create_migrator: Callable[[], Any]


def sample_file_content(relative_dir: str, filename: str) -> str:
    """Return deterministic sample file contents."""
    return f"Sample data for {filename}\n" f"Directory: {relative_dir}\n" "Generated by migrate_v2.py --test\n"


def materialize_sample_tree(root: Path):
    """Create nested directories and files for smoke testing."""
    file_count = 0
    dir_count = 0
    total_bytes = 0
    root.mkdir(parents=True, exist_ok=True)
    for relative_dir, files in _SAMPLE_STRUCTURE.items():
        dir_path = root / relative_dir
        dir_path.mkdir(parents=True, exist_ok=True)
        dir_count += 1
        for filename in files:
            file_path = dir_path / filename
            content = sample_file_content(relative_dir, filename)
            file_path.write_text(content, encoding="utf-8")
            file_count += 1
            total_bytes += file_path.stat().st_size
    return file_count, dir_count, total_bytes


def create_sample_objects_in_s3(s3_client, bucket: str):
    """Upload the sample structure directly to S3 and return manifest + stats."""
    manifest = {}
    files_created = 0
    total_bytes = 0
    for relative_dir, files in _SAMPLE_STRUCTURE.items():
        for filename in files:
            key = f"{relative_dir}/{filename}"
            content = sample_file_content(relative_dir, filename).encode("utf-8")
            s3_client.put_object(Bucket=bucket, Key=key, Body=content)
            hasher = hashlib.sha256()
            hasher.update(content)
            manifest[key] = hasher.hexdigest()
            files_created += 1
            total_bytes += len(content)
    dir_count = len(_SAMPLE_STRUCTURE)
    return manifest, files_created, dir_count, total_bytes


def manifest_directory(root: Path):
    """Return a manifest of files and hashed contents for validation."""
    manifest = {}
    for file_path in sorted(root.rglob("*")):
        if file_path.is_file():
            hasher = hashlib.sha256()
            hasher.update(file_path.read_bytes())
            manifest[file_path.relative_to(root).as_posix()] = hasher.hexdigest()
    return manifest


class BackupVerificationError(RuntimeError):
    """Raised when the smoke-test backup verification fails."""

    def __init__(self) -> None:
        super().__init__("Backup verification failed - manifests do not match")


def ensure_matching_manifests(manifest_before, manifest_after) -> None:
    """Raise when backup manifests differ."""
    if manifest_before != manifest_after:
        raise BackupVerificationError()


__all__ = [
    "BackupVerificationError",
    "SmokeTestDeps",
    "create_sample_objects_in_s3",
    "ensure_matching_manifests",
    "manifest_directory",
    "materialize_sample_tree",
    "sample_file_content",
]
